# MAIN datapipeline configuration file

default_profile:
  # mode is (si_split|si_1to1|ex2an)
  mode: si_split
  # reader and writer classes
  reader: excel
  writer: csv
  # if the data should be validated or modified 
  transform: true
  # for split mode, which sheet of source excel should be taken
  default_sheet_nr: 0
  fnames_src: data
  fnames_dst: config

floworg_equip:
  storage_src: yamldir
  storage_dst: yaml
  writer: excel
  sub: floworg_equip
  sub_in: floworg_equip
  sub_out: floworg_equip

wanted_bmarks_eip:
  storage_src: json
  storage_dst: json
  writer: excel
  sub: bmarks_eip
  sub_in: bmarks_eip
  sub_out: bmarks_eip

treemod_bmarks:
  reader: excel  # not used, but why not use reader for the src json??
  writer: excel
  storage_src: json
  storage_dst: yaml
  sub: bmarks
  sub_in: bmarks
  sub_out: bmarks

keepass_build:
  storage_src: kdbx
  storage_dst: kdbx
  reader: excel
  writer: excel
  sub: keepass
  sub_in: SI
  sub_out: keepass

# ServerInfrastruktur.xslx splitten 
excel_copy:
  mode: si_1to1
  reader: excel
  writer: excel

excel_split_first:
  mode: si_split
  reader: excel
  writer: excel

split_duckdb_to_csv:
  mode: si_split
  reader: duckdb
  writer: csv

ex2ansible:
  mode: ex2an
  reader: excel
  writer: ansible
  sub_in: SI
  sub_out: ansible

# in 1to1 mode, fnames are fetched from the reader

# in split mode, we need to set fnames for all output sheets !
